{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='margin-top:8px'><img src='data:image/png;base64,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' width='30px' height='35px' style='display: inline-block; padding-right: 10px'></img><span>Lightning initialized</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to server at https://desolate-castle-4773.herokuapp.com\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "(function e(t,n,r){function s(o,u){if(!n[o]){if(!t[o]){var a=typeof require==\"function\"&&require;if(!u&&a)return a(o,!0);if(i)return i(o,!0);throw new Error(\"Cannot find module '\"+o+\"'\")}var f=n[o]={exports:{}};t[o][0].call(f.exports,function(e){var n=t[o][1][e];return s(n?n:e)},f,f.exports,e,t,n,r)}return n[o].exports}var i=typeof require==\"function\"&&require;for(var o=0;o<r.length;o++)s(r[o]);return s})({1:[function(require,module,exports){\n",
       "window.lightning = window.lightning || {};\n",
       "var lightningCommMap = {};\n",
       "var IPython = window.IPython;\n",
       "\n",
       "var readCommData = function(commData, field) {\n",
       "    try {\n",
       "        return commData.content.data[field];\n",
       "    } catch (err) {\n",
       "        return;\n",
       "    }\n",
       "};\n",
       "\n",
       "\n",
       "var init_comm = function() {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('lightning', function(comm, data) {\n",
       "        var id = readCommData(data, 'id');\n",
       "        lightningCommMap[id] = comm;\n",
       "    });\n",
       "\n",
       "    window.lightning.comm_map = lightningCommMap;\n",
       "}\n",
       "\n",
       "\n",
       "if(IPython && IPython.notebook) {\n",
       "\n",
       "    if(IPython.notebook.kernel) {\n",
       "        init_comm();\n",
       "    }\n",
       "\n",
       "    IPython.notebook.events.on('kernel_connected.Kernel', init_comm);\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "},{}]},{},[1])\n",
       "//# sourceMappingURL=data:application/json;base64,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\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import logging, gensim, bz2\n",
    "from gensim import corpora, models, similarities\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import itertools\n",
    "import random\n",
    "exclude = set(string.punctuation)\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (18.0, 18.0)\n",
    "import numpy as np\n",
    "from lightning import Lightning\n",
    "from numpy import random, asarray\n",
    "import networkx as nx\n",
    "from __future__ import division\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "lgn = Lightning(ipython=True, host='https://desolate-castle-4773.herokuapp.com/')\n",
    "lgn.set_size(size='large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_names = pickle.load( open( \"topics_names.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadText(path):\n",
    "    soup = BeautifulSoup( open(path), 'lxml')\n",
    "    s = ' '\n",
    "    for string in soup.find_all(\"source\"):\n",
    "        s += ' ' + string.string\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = loadText(u\"all_source_texts/Turkish_Basketball_Clubs_Evaluationprogramme15-16_Repucom_150612_Übersetzten-en-tr-T.mxliff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "    \n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    # tokenize + punctuation\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # remove punctuation\n",
    "    text = tokenizer.tokenize(text)\n",
    "    # remove stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    stops = stopwords.words('english')\n",
    "    text = [ w for w in text if w.lower() not in stops]\n",
    "    # Exclude numbers\n",
    "    text = [s for s in text if not re.search(r'\\d',s)]\n",
    "    #remove word with less than 3letters\n",
    "    text = [s for s in text if len(s) > 2]\n",
    "    # stemmer\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "#     stemmer = SnowballStemmer(\"english\")\n",
    "    text =  [(lmtzr.lemmatize(t)) for t in text] \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens =  tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load all texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "filelist = []\n",
    "root = u\"all_source_texts/\"\n",
    "for filename in os.listdir(root):\n",
    "    filename = root + filename\n",
    "    filelist .append(filename.encode(sys.getfilesystemencoding()))\n",
    "\n",
    "i = 0\n",
    "corpus_tokens = {}\n",
    "for f in filelist:\n",
    "    corpus_tokens[f] = tokenize(loadText( f ))\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel.load(u'lda/wikipedia_lda', mmap='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = { f:lda.id2word.doc2bow(corpus_tokens[f]) for f in corpus_tokens}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# update lda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.update( corpus.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dictionary.save('dict_corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in filelist:\n",
    "    topic = lda.get_document_topics(corpus[f])\n",
    "    print(f + \"  \" + str(sum([ s[1] for s in topic])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get topic distribution from a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def topicsFromFiles( path ):\n",
    "    tokens = tokenize(loadText( path ))\n",
    "    bow = lda.id2word.doc2bow( tokens )\n",
    "    return lda.get_document_topics(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = random.choice(filelist)\n",
    "topicsFromFiles(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# store topics words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# translate the lda topics in dictionnaries\n",
    "i = 0\n",
    "n = 100 # number of words per topics\n",
    "topics = dict()\n",
    "wordset = set()\n",
    "for i in range(0,100):\n",
    "    a = lda.print_topic(i, n)\n",
    "    topic = dict()\n",
    "#     print a\n",
    "    pairs = a.split( ' + ')\n",
    "    for p in pairs:\n",
    "        pair = p.split('*')\n",
    "#         print pair[0]\n",
    "        value = float(pair[0])\n",
    "        key = unicode(pair[1])\n",
    "        topic[key] = value\n",
    "        wordset.add(key)\n",
    "    topics[i] = topic   \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show topic as a bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "matplotlib.rcParams['figure.figsize'] = (15.0, 8.0)\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def WordCloudTopic( i ):\n",
    "    # Generate a word cloud image\n",
    "    wc = WordCloud(background_color=\"white\", max_words=2000,\n",
    "               max_font_size=40, random_state=42)\n",
    "    wordcloud = wc.generate_from_frequencies(topics[i].items())    \n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "def printCloud( frequencies, imagePath = \"image.jpg\" ):\n",
    "    # Read the whole text.\n",
    "\n",
    "    # Read the whole text.\n",
    "#     text =\" \".join(all_words) #open(path.join(d, 'alice.txt')).read()\n",
    "#     print \"text joined\"\n",
    "\n",
    "    # read the mask / color image\n",
    "    # taken from http://jirkavinse.deviantart.com/art/quot-Real-Life-quot-Alice-282261010\n",
    "    alice_coloring = np.array(Image.open(imagePath))\n",
    "\n",
    "    wc = WordCloud(background_color=\"white\", max_words=200, mask=alice_coloring,\n",
    "                   stopwords=STOPWORDS.add(\"said\"),\n",
    "                   max_font_size=300)\n",
    "    # generate word cloud\n",
    "    wc.generate_from_frequencies(frequencies)\n",
    "\n",
    "    # create coloring from image\n",
    "    image_colors = ImageColorGenerator(alice_coloring)\n",
    "\n",
    "    # show\n",
    "    # plt.imshow(wc)\n",
    "    # plt.axis(\"off\")\n",
    "    # plt.figure()\n",
    "    # recolor wordcloud and show\n",
    "    # we could also give color_func=image_colors directly in the constructor\n",
    "    plt.imshow(wc.recolor(color_func=image_colors))\n",
    "    plt.axis(\"off\")\n",
    "    plt.figure()\n",
    "    # plt.imshow(alice_coloring, cmap=plt.cm.gray)\n",
    "    # plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (15.0, 8.0)\n",
    "WordCloudTopic(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# get the topic and the score for a unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTopicFromWord( unique_word ):\n",
    "    bow = lda.id2word.doc2bow( [unique_word] )\n",
    "    topic = lda.get_document_topics(bow)\n",
    "    if len(topic):\n",
    "        return topic[0][0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "print getTopicFromWord('have')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import *\n",
    "from scipy import *\n",
    "V = dok_matrix((0,100), dtype=float32)\n",
    "\n",
    "def getTopic_n_topWordsWithinTopics( path ):\n",
    "    tokens = tokenize(loadText( path ))\n",
    "    bow = lda.id2word.doc2bow( tokens )\n",
    "    topics =  lda.get_document_topics(bow)\n",
    "    complex_topic = dict()\n",
    "    complex_topic['topics'] = topics\n",
    "    details = dict()\n",
    "    max_details = 0\n",
    "    for t in tokens:\n",
    "        i = getTopicFromWord( t )\n",
    "        if i != None:\n",
    "            if i not in details.keys():\n",
    "                details[i]  = dict()\n",
    "            if t not in details[i].keys():\n",
    "                details[i][t] = 1\n",
    "            else:\n",
    "                details[i][t] += 1\n",
    "                if details[i][t] > max_details:\n",
    "                    max_details = details[i][t]\n",
    "                \n",
    "    V = dok_matrix((1,100), dtype=float32)\n",
    "    for t in topics:\n",
    "        V[0,t[0]] = t[1]\n",
    "    \n",
    "    complex_topic['semantic_vector'] = V\n",
    "    complex_topic['details'] = details\n",
    "    complex_topic['max_details'] = max_details\n",
    "    return complex_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# complex_topics = dict()\n",
    "# for f in filelist:\n",
    "#     complex_topics[f] = getTopic_n_topWordsWithinTopics(f)\n",
    "    \n",
    "pickle.dump( complex_topics, open( \"topics_from_files.p\", \"wb\" ) )\n",
    "\n",
    "complex_topics = pickle.load( open( \"topics_from_files.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print loadText(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# display main topics and the top words from the text within a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = random.choice(filelist)\n",
    "print f\n",
    "x = complex_topics[f]\n",
    "x_topics = x['topics']\n",
    "x_details = x['details']\n",
    "for i in sorted(x_topics, key=lambda tup: tup[1], reverse = True):\n",
    "    print \"score: %s \" %i[1] + \"| topic: %s\" %topic_names[i[0]] + \" | id topic: %s\" %i[0]\n",
    "    d = x_details[i[0]]\n",
    "    for w in sorted(d, key=d.get, reverse=True)[:10]:\n",
    "        print \"-----  \" + str(w) + \": \" + str(d[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm(v):\n",
    "    s = 0\n",
    "#     it = v.itervalues()\n",
    "#     while it:\n",
    "#         s+= it\n",
    "#         it.next()\n",
    "    for d in v.itervalues():\n",
    "        s+= d**2\n",
    "    return sqrt(s)\n",
    "\n",
    "def similarity( a, b):\n",
    "    # cosine similarity\n",
    "    p = (a.dot(b.transpose()) / (norm(a) * norm(b))).data\n",
    "    if len(p):\n",
    "        return p[0]\n",
    "\n",
    "\n",
    "def closestFile( path ):\n",
    "    x = getTopic_n_topWordsWithinTopics(path)\n",
    "    u = x['semantic_vector']\n",
    "    print type(u)\n",
    "    similarities = dict()\n",
    "    for s in complex_topics:\n",
    "        v = complex_topics[s]['semantic_vector']\n",
    "        similarities[s] = similarity(v,u)     \n",
    "    k = 1\n",
    "    for i in sorted(similarities.items(), key=lambda x: x[1])[::-1][:15]:\n",
    "        print str(k) + \"  |  \"  + str(i)\n",
    "        k += 1\n",
    "        \n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = u\"all_source_texts/Turkish_Basketball_Clubs_Evaluationprogramme15-16_Repucom_150612_Übersetzten-en-tr-T.mxliff\"\n",
    "closestFile( path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = u\"all_source_texts/1920-0002 Princess Interiors-en-fr_ca-CR.mxliff\"\n",
    "closestFile( path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# display the results as a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRESHOLD = 2\n",
    "min_score = 0.03 #5%\n",
    "\n",
    "colours = []\n",
    "colours.append( (26, 188, 156))\n",
    "colours.append( (52, 152, 219))\n",
    "colours.append( (155, 89, 182))\n",
    "colours.append( (241, 196, 15))\n",
    "colours.append( (231, 76, 60))\n",
    "colours.append( (46, 204, 113))\n",
    "colours.append( (230, 126, 34))\n",
    "colours.append( (149, 165, 166))\n",
    "colours.append( (52, 73, 94))\n",
    "\n",
    "class idGenerator:\n",
    "    def __init__(self):\n",
    "        self.id = 0\n",
    "    def get(self):\n",
    "        self.id += 1\n",
    "        return self.id - 1\n",
    "    \n",
    "    \n",
    "class node:\n",
    "    def __init__(self, index, label, weight = 30., color = (236, 240, 241)):\n",
    "        self.index = index\n",
    "        self.label = label\n",
    "        self.childs = []\n",
    "        self.weight = weight\n",
    "        self.connections = None\n",
    "        self.groups = []\n",
    "        self.color = color\n",
    "    def addChild( self, c ):\n",
    "        self.childs.append( c )\n",
    "        \n",
    "    def getGraph(self):\n",
    "        self.connections = []\n",
    "        self.groups.append(0)\n",
    "        graph = nx.Graph()\n",
    "        graph.add_node( self.index, weight = self.weight, label = self.label, color = self.color)\n",
    "        k = 0\n",
    "        for c in self.childs: \n",
    "            color = colours[k]\n",
    "            group = []\n",
    "            k+=1\n",
    "            if k >= len(colours):\n",
    "                k = 0\n",
    "            self.groups.append( k )\n",
    "            graph.add_node( c.index, weight = c.weight, label = c.label, color = c.color)\n",
    "            graph.add_edge(self.index, c.index )\n",
    "            self.connections.append([ self.index, c.index, 0.1 ])\n",
    "            for cc in c.childs:\n",
    "                graph.add_node( cc.index, weight = cc.weight, label = cc.label, color = cc.color)\n",
    "                graph.add_edge(c.index, cc.index )\n",
    "                self.connections.append([ c.index, cc.index, 0. ])\n",
    "                self.groups.append( 0 )\n",
    "        return graph\n",
    "    \n",
    "    def getPos(self, sizeX = 5, sizeY = 15):\n",
    "        X = []\n",
    "        Y = []\n",
    "        X.append(0)\n",
    "        Y.append(  sizeY/2 )\n",
    "        n = len(self.childs)\n",
    "        cur = 0\n",
    "        offset = 0.5\n",
    "        for i in range(0, n):   \n",
    "            c = self.childs[i]\n",
    "            m = len(c.childs)\n",
    "            w = c.weight\n",
    "            X.append(3 * sizeX / 5)\n",
    "            Y.append( cur  )\n",
    "            offsetY = w * sizeY\n",
    "            for j in range(0,m):\n",
    "                # set word nodes position\n",
    "                X.append( sizeX )# + 0.5* math.cos( 2 * 3.12 * j / m) )\n",
    "                Y.append( cur + offset +  j/m * w * (sizeY - offset) - offsetY / 2)#+ 0.5 * math.sin( 2 * 3.12 * j / m))\n",
    "            \n",
    "            # set topics nodes postions\n",
    "            cur += offsetY\n",
    "            \n",
    "        Y = [ sizeY - y for y in Y]      \n",
    "        return X,Y\n",
    "    \n",
    "def makeGraphFromText( f ):\n",
    "    id = idGenerator()\n",
    "    x = complex_topics[f]\n",
    "    x_topics = x['topics']\n",
    "    x_details = x['details']\n",
    "    max_details = x['max_details']\n",
    "    text_node = node( id.get() , f)\n",
    "    k = 1\n",
    "    for i in sorted(x_topics, key=lambda tup: tup[1], reverse = True):\n",
    "        if i[1] > min_score:\n",
    "            color = colours[k]\n",
    "            k+=1\n",
    "            if k >= len(colours):\n",
    "                k = 0\n",
    "            topic_node = node( id.get(), topic_names[i[0]], weight = 50 * i[1], color = color )\n",
    "            d = x_details[i[0]]\n",
    "            # add words from text\n",
    "            for w in sorted(d, key=d.get, reverse=True)[:10]:\n",
    "                if d[w] > 1:\n",
    "                    detail_node = node( id.get(), str(str(w) + \": \" +  str(d[w])), weight =  18 * float(d[w]) / float(max_details))\n",
    "                    topic_node.addChild(detail_node)\n",
    "            # add words from topics\n",
    "            for w in sorted(topics[i[0]], key=topics[i[0]].get, reverse=True)[:20]: \n",
    "                detail_node = node( id.get(), str(str(w) + \": \" +  str(topics[i[0]][w])), weight =  300 * float(topics[i[0]][w]), color = color )\n",
    "                topic_node.addChild(detail_node)\n",
    "\n",
    "            text_node.addChild( topic_node )\n",
    "            \n",
    "    return text_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://desolate-castle-4773.herokuapp.com/sessions/\n",
    "\n",
    "f = random.choice(filelist)\n",
    "G = makeGraphFromText( f )\n",
    "\n",
    "print f\n",
    "graph = G.getGraph()\n",
    "connections = G.connections\n",
    "groups = G.groups\n",
    "X,Y = G.getPos(2 , 30)\n",
    "mat = asarray(nx.adjacency_matrix(graph).todense())\n",
    "weights =  nx.get_node_attributes(graph, 'weight').values()\n",
    "labels = nx.get_node_attributes(graph, 'label').values()\n",
    "colors = nx.get_node_attributes(graph, 'color').values()\n",
    "colors = np.vstack(colors)\n",
    "\n",
    "lgn.force(mat, size = weights, labels = labels, color = colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# topic distribution along a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_seq(seq, num_pieces):\n",
    "    start = 0\n",
    "    for i in xrange(num_pieces):\n",
    "        stop = start + len(seq[i::num_pieces])\n",
    "        yield seq[start:stop]\n",
    "        start = stop\n",
    "        \n",
    "def getTopicsDistributionWithinTheText(path, words = 300 ):\n",
    "    text = loadText(path)\n",
    "    tokens = tokenize(text)\n",
    "    steps = int(len(tokens) / words)\n",
    "    if steps < 2:\n",
    "        steps = 2\n",
    "    bow = lda.id2word.doc2bow(tokens)\n",
    "    global_scores = lda.get_document_topics(bow)\n",
    "    scores = dict()\n",
    "    for i in sorted(global_scores, key=lambda tup: tup[1], reverse = True):\n",
    "        if i[1] > min_score:\n",
    "            scores[i[0]] = []\n",
    "    chunks = split_seq(tokens, steps)\n",
    "    i = 1\n",
    "    for c in chunks:\n",
    "        bow = lda.id2word.doc2bow(c)\n",
    "        score = lda.get_document_topics(bow)\n",
    "        for s in score:\n",
    "            if s[0] in scores.keys():\n",
    "                scores[s[0]].append(s[1])\n",
    "                \n",
    "        for s in scores:\n",
    "            if len(scores[s]) < i:\n",
    "                scores[s].append(0)\n",
    "        i += 1\n",
    "        \n",
    "    return scores, global_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "print f\n",
    "\n",
    "def displayTopicsDistributionWithinTheText(f, chunk_length = 300):\n",
    "\n",
    "    distribAlongText, global_scores = getTopicsDistributionWithinTheText(f, chunk_length)\n",
    "\n",
    "    global_scores =  sorted(global_scores, key=lambda tup: tup[1], reverse = True)\n",
    "    scores = []\n",
    "    for g in global_scores:\n",
    "        if g[0] in distribAlongText.keys():\n",
    "            scores.append( distribAlongText[g[0]])\n",
    "    k = 1\n",
    "    \n",
    "    # draw pie chart\n",
    "    values = []\n",
    "    labels = []\n",
    "    for s in global_scores:\n",
    "        if s[1] > min_score:\n",
    "            values.append(s[1])\n",
    "            labels.append(topic_names[s[0]])\n",
    "    matplotlib.rcParams['figure.figsize'] = (8.0, 8.0)  \n",
    "    plt.pie(values,  labels=labels, colors = [ [1.0 / 255.0 * c for c in cc] for cc in colours[1:]])\n",
    "    plt.show()\n",
    "    matplotlib.rcParams['figure.figsize'] = (15.0, 8.0)\n",
    "    # print type(scores)\n",
    "    x = range( 0, len(scores[0]))\n",
    "    x = [ chunk_length * i for i in x]\n",
    "    for s in scores:\n",
    "#         xnew = np.arange(min(x), max(x))\n",
    "#         ynew = interpolate.splev(xnew,tck,der=0)\n",
    "#         fun = interp1d(x, s, kind='cubic')\n",
    "    #     plt.plot(xnew, fun(xnew),linewidth = 5, color = [ 1.0 / 255.0 * c for c in colours[k]], alpha = 0.6)\n",
    "    #     plt.plot(xnew, ynew,linewidth = 5, color = [ 1.0 / 255.0 * c for c in colours[k]], alpha = 0.6)\n",
    "        plt.plot(x, s,linewidth = 5, color = [ 1.0 / 255.0 * c for c in colours[k]], alpha = 0.6)\n",
    "        plt.fill_between(x, s,linewidth = 5, color = [ 1.0 / 255.0 * c for c in colours[k]], alpha = 0.3)\n",
    "        k += 1\n",
    "        if k >= len(colours):\n",
    "            k = 0\n",
    "\n",
    "    plt.ylabel('proportion')\n",
    "    plt.xlabel('number of words')\n",
    "    plt.show()\n",
    "    \n",
    "displayTopicsDistributionWithinTheText(f, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Complex words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getlongwords( path ):\n",
    "    text = loadText(path)\n",
    "    tokens = tokenize(text)\n",
    "    f = nltk.FreqDist(tokens)\n",
    "    result = dict()\n",
    "    tokens = sorted(set(tokens), key=lambda w: len(w), reverse = True)\n",
    "    for w in sorted(set(tokens), key=lambda w: len(w), reverse = True)[:15] :\n",
    "        print w + \": \" +  str(f[w])\n",
    "        \n",
    "    return tokens\n",
    " \n",
    "# path = u\"all_source_texts/1920-0002 Princess Interiors-en-fr_ca-CR.mxliff\"\n",
    "path = u\"all_source_texts/Turkish_Basketball_Clubs_Evaluationprogramme15-16_Repucom_150612_Übersetzten-en-tr-T.mxliff\"\n",
    "\n",
    "r = getlongwords( path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct a corpus to compute frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "# print  \"corpus with: %s words (%s distincts) \" %(len(reuters.words()), len(set(reuters.words())))\n",
    "\n",
    "# freq_english = nltk.FreqDist(reuters.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_english[\"satisfaction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getMostSignificativeWords( path ):\n",
    "    text = loadText(path)\n",
    "    tokens = tokenize(text)\n",
    "    f = nltk.FreqDist(tokens)\n",
    "    word_set = set(tokens)\n",
    "    result = dict()\n",
    "    for w in word_set:\n",
    "        score = len(w)  / np.log(2 +  freq_english[w])\n",
    "        result[w] = score\n",
    "    return result\n",
    "\n",
    "def CloudFromSignificantWords( path, img_path ):\n",
    "    words = getMostSignificativeWords(path)\n",
    "    items = []\n",
    "    for w in sorted(words, key=words.get, reverse=True)[:100]:\n",
    "        items.append( (w, words[w]))\n",
    "    printCloud( items, img_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mostSignificative = getMostSignificativeWords(path)\n",
    "# for w in sorted(mostSignificative, key=mostSignificative.get, reverse=True)[:10]:\n",
    "#     print  str(w) + \": \" + str(mostSignificative[w])\n",
    "\n",
    "CloudFromSignificantWords(path, \"image.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://pypi.python.org/pypi/textstat/\n",
    "\n",
    "from textstat.textstat import textstat\n",
    "\n",
    "difficulties = dict()\n",
    "for f in filelist:\n",
    "    txt = unicode(loadText(f))\n",
    "    try:\n",
    "        difficulties[f] = textstat.automated_readability_index((txt))\n",
    "    except:\n",
    "        print \"fail:\" + str(f)\n",
    "        \n",
    "print textstat.automated_readability_index((f))\n",
    "print loadText(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.fill_between(range(0,len(difficulties)),sorted(difficulties.values(), reverse = True), color = [ 1.0 / 255.0 * c for c in colours[0]], alpha = 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test all on a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = u\"all_source_texts/babyshop texter vecka 33-en-fi-T.mxliff\"\n",
    "# path = u\"all_source_texts/Re-Nutriv Script_Final Translation Template_deadline 20. april-en-da-T.mxliff\"\n",
    "path = random.choice(filelist)\n",
    "print loadText(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"mingningful words\"\n",
    "CloudFromSignificantWords(path, \"image_gradient.jpg\")\n",
    "print \"topic distribution\"\n",
    "displayTopicsDistributionWithinTheText(path, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# path = u\"all_source_texts/babyshop texter vecka 33-en-fi-T.mxliff\"\n",
    "# path = u\"all_source_texts/Re-Nutriv Script_Final Translation Template_deadline 20. april-en-da-T.mxliff\"\n",
    "G = makeGraphFromText( path )\n",
    "\n",
    "print \"topic graph\"\n",
    "print path\n",
    "graph = G.getGraph()\n",
    "connections = G.connections\n",
    "groups = G.groups\n",
    "X,Y = G.getPos(2 , 30)\n",
    "mat = asarray(nx.adjacency_matrix(graph).todense())\n",
    "weights =  nx.get_node_attributes(graph, 'weight').values()\n",
    "labels = nx.get_node_attributes(graph, 'label').values()\n",
    "colors = nx.get_node_attributes(graph, 'color').values()\n",
    "colors = np.vstack(colors)\n",
    "\n",
    "lgn.force(mat, size = weights, labels = labels, color = colors)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mostSignificative = getMostSignificativeWords(path)\n",
    "# for w in sorted(mostSignificative, key=mostSignificative.get, reverse=True)[:30]:\n",
    "#     print  str(w) + \": \" + str(mostSignificative[w])\n",
    "    \n",
    "# drawNX(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# draw graph with networkX\n",
    "\n",
    "\n",
    "def drawNX(graph):\n",
    "    i = 0\n",
    "    matplotlib.rcParams['figure.figsize'] = (20.0, 15.0)\n",
    "    labelsNX = dict()\n",
    "    for l in labels:\n",
    "        labelsNX[i] = l\n",
    "        i += 1\n",
    "\n",
    "    i = 0\n",
    "    weightsNX = dict()\n",
    "    for l in weights:\n",
    "        weightsNX[i] = l\n",
    "        i += 1\n",
    "\n",
    "    pos=nx.spring_layout(graph)\n",
    "    nx.draw(graph, labels = labelsNX,pos = pos, edge_color = 'gray', node_color  = [ [(1.0/255.0) * cc for cc in c]  for c in colors], node_size  = [ 80 * w for w in weights], alpha = 0.7, font_size = 18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# graph bundle\n",
    "# from numpy import random, ceil, array\n",
    "# import numpy as np\n",
    "# x = random.randn(10)\n",
    "# y = random.randn(10)\n",
    "# mat = random.rand(10,10)\n",
    "# mat[mat>0.75] = 0\n",
    "\n",
    "# mat = np.zeros((len(connections), 3))\n",
    "# # lgn = Lightning(ipython=True, host='http://public.lightning-viz.org')\n",
    "# # lgn.set_size(size='large')\n",
    "# print len(x)\n",
    "# print len(connections)\n",
    "# for i in range( 0, len(connections)):\n",
    "#     mat[i,0] = connections[i][0]\n",
    "#     mat[i,1] = connections[i][1]\n",
    "#     mat[i,2] = connections[i][2]\n",
    "    \n",
    "# mat2 = np.ones((len(X), len(Y)) )\n",
    "# # print mat2\n",
    "# # print X\n",
    "# # print Y\n",
    "\n",
    "# lgn.graphbundled(X,Y,  mat, size = weights, labels = labels, color = colours)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqs = nltk.FreqDist(nltk.corpus.gutenberg.words())\n",
    "    \n",
    "printCloud( freqs.most_common(500), \"image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(topics[94].items(), key=lambda x: x[1],reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_names = dict()\n",
    "for i in range(0,100):\n",
    "    topic_names[i] = i\n",
    "    \n",
    "topic_names[0] = \"space - sci fi\"\n",
    "topic_names[1] = \"american football\"\n",
    "topic_names[2] = \"design\"\n",
    "topic_names[3] = \"japan\"\n",
    "topic_names[4] = \"american eduacation\"\n",
    "topic_names[5] = \"north carolina\"\n",
    "topic_names[6] = \"cities - airports ?\"\n",
    "\n",
    "topic_names[72] = 'IT'\n",
    "topic_names[78] = 'positive words - marketing'\n",
    "topic_names[2] = 'design - production'\n",
    "topic_names[89] = 'nutrition'\n",
    "topic_names[41] = 'military - emergency'\n",
    "topic_names[64] = 'colors - nature'\n",
    "topic_names[83] = 'food'\n",
    "topic_names[55] = 'clothes - materials'\n",
    "topic_names[77] = 'TV - people - fashion'\n",
    "topic_names[29] = 'music'\n",
    "topic_names[81] = 'human ressources'\n",
    "topic_names[97] = 'science'\n",
    "topic_names[40] = 'health'\n",
    "topic_names[73] = 'mountains - nature'\n",
    "topic_names[16] = 'buildings'\n",
    "topic_names[25] = 'business - distribution'\n",
    "topic_names[67] = 'medieval'\n",
    "topic_names[86] = 'childish - games'\n",
    "topic_names[94] = 'classical music - composers'\n",
    "\n",
    "pickle.dump( topic_names, open( \"topics_names.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics_distrib = dict()\n",
    "\n",
    "for t in complex_topics:\n",
    "    for s in complex_topics[t]['topics']:\n",
    "        if s[0] in topics_distrib.keys():\n",
    "            topics_distrib[s[0]] += s[1]\n",
    "        else:\n",
    "            topics_distrib[s[0]] = s[1]\n",
    "            \n",
    "for w in sorted(topics_distrib, key=topics_distrib.get, reverse=True)[:20]:\n",
    "    print str(w) + \": \" + str(topics_distrib[w])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://desolate-castle-4773.herokuapp.com/visualizations/88577a1f-8dcf-4891-9bfe-a6c00d7aa2f4/iframe/” width=\"1200\" height=\"500\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare source and target with sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from translate import translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "503 Server Error: Service Unavailable for url: https://translate.google.com/translate_a/single?oe=utf-8&q=Hello+World%21&tl=fr&client=a&sl=en&dt=t&ie=utf-8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5c4245804cee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Hello World!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Flet-BerliacYannis/anaconda/lib/python2.7/site-packages/translate/translator.pyc\u001b[0m in \u001b[0;36mconnection\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mcleanup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr',(?=,)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Flet-BerliacYannis/anaconda/lib/python2.7/site-packages/requests/models.pyc\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 503 Server Error: Service Unavailable for url: https://translate.google.com/translate_a/single?oe=utf-8&q=Hello+World%21&tl=fr&client=a&sl=en&dt=t&ie=utf-8"
     ]
    }
   ],
   "source": [
    "translator('en', 'fr', 'Hello World!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('sentiment.txt', 'r').read().split(\"\\n\")[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiment={}\n",
    "i=0\n",
    "while i < len(f)-1:\n",
    "    sentiment[f[i].split()[0]]=f[i].split()[1]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadTextTarget(path):\n",
    "    soup = BeautifulSoup( open(path), 'lxml')\n",
    "    s = ' '\n",
    "    for string in soup.find_all(\"Target\"):\n",
    "        s += ' ' + string.string\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = loadText(u\"all_source_texts/_WL-Contact_Technical_And_Functional_Proposal-1-en-de-T.mxliff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = loadTextTarget(u\"all_source_texts/_WL-Contact_Technical_And_Functional_Proposal-1-en-de-T.mxliff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "503 Server Error: Service Unavailable for url: https://translate.google.com/translate_a/single?oe=utf-8&q=raining&tl=de&client=a&sl=en&dt=t&ie=utf-8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-262aa9e8b290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'de'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Flet-BerliacYannis/anaconda/lib/python2.7/site-packages/translate/translator.pyc\u001b[0m in \u001b[0;36mconnection\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mcleanup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr',(?=,)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Flet-BerliacYannis/anaconda/lib/python2.7/site-packages/requests/models.pyc\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 503 Server Error: Service Unavailable for url: https://translate.google.com/translate_a/single?oe=utf-8&q=raining&tl=de&client=a&sl=en&dt=t&ie=utf-8"
     ]
    }
   ],
   "source": [
    "translate=[]\n",
    "for key in sentiment:\n",
    "    translate.append(translator('en', 'de', key)[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
